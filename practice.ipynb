{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100개의 feature을 통해서 label y를 예측하기 위한 프로세스 마련. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.datasets import make_moons\n",
    "def get_noisy_two_moons(n_samples=1000, n_feats=100, noise_twomoon=0.1, noise_nuisance=1.0, seed_=1234):\n",
    "    X, Y = make_moons(n_samples=n_samples, noise=noise_twomoon, random_state=seed_)\n",
    "    np.random.seed(seed_)\n",
    "    N = np.random.normal(loc=0., scale=noise_nuisance, size=[n_samples, n_feats-2])\n",
    "    X = np.concatenate([X, N], axis=1)\n",
    "\n",
    "    Y_onehot = np.zeros([n_samples, 2])\n",
    "    Y_onehot[Y == 0, 0] = 1\n",
    "    Y_onehot[Y == 1, 1] = 1\n",
    "\n",
    "    return X, Y, Y_onehot\n",
    "\n",
    "##function that arbritrally adds up the feature (there are only two important features but, in order to check for the robustness of the method,  add up all the features)\n",
    "def get_blockcorr(X, block_size=10, noise_=0.5, seed_=1234): \n",
    "    '''\n",
    "        noise 0.5 ~ 0.85 correlation\n",
    "        noise 1.0 ~ 0.66 correlation\n",
    "    '''\n",
    "    for p in range(X.shape[1]):\n",
    "        np.random.seed(seed_ + p)\n",
    "        tmp   = X[:, [p]] + np.random.normal(loc=0., scale=noise_, size=[X.shape[0], block_size-1])\n",
    "\n",
    "        if p == 0:\n",
    "            X_new = np.concatenate([X[:, [p]], tmp], axis=1)\n",
    "        else:\n",
    "            X_new = np.concatenate([X_new, X[:, [p]], tmp], axis=1)    \n",
    "    return X_new   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## L is a lower diagonal form of a covariance matrix, and in this case we assume there is no removement of index. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def mask_generation(mb_size_, pi_, L):\n",
    "    ## mb_size is a size of minibatch, pi_ as a hyper parameter that controls the probability, \n",
    "    epsilon = np.random.normal(loc=0., scale=1., size=[np.shape(L)[0], mb_size_])\n",
    "    g=np.matmul(L, epsilon)\n",
    "    m = (1/2 * (1 + scipy.special.erf(g/np.sqrt(2)) ) < pi_).astype(float).T\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let me create any correlation matrix\n",
    "def correlation_matrix_generator(n_feats, seed_=1234):\n",
    "    np.random.seed(seed_)\n",
    "    L = np.random.normal(loc=0., scale=1., size=[n_feats, n_feats])\n",
    "    L = np.tril(L)\n",
    "    return L\n",
    "L=correlation_matrix_generator(100)\n",
    "m=mask_generation(32, 0.5, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_generation(mb_size_, pi_):\n",
    "    '''\n",
    "        Phi(x; mu, sigma) = 1/2 * (1 + erf( (x-mu)/(sigma * sqrt(2)) )) \n",
    "        --> Phi(x; 0,1)   = 1/2 * (1 + erf( x/sqrt(2) )) \n",
    "    '''\n",
    "    if len(remove_idx) == 0:\n",
    "        epsilon = np.random.normal(loc=0., scale=1., size=[np.shape(L)[0], mb_size_])\n",
    "        g       = np.matmul(L, epsilon)\n",
    "    else:\n",
    "        present_idx = [i for i in range(x_dim) if i not in remove_idx]\n",
    "        epsilon     = np.random.normal(loc=0., scale=1., size=[np.shape(L)[0], mb_size_])\n",
    "        g2      = np.random.normal(loc=0., scale=1., size=[len(remove_idx), mb_size_])\n",
    "        g1      = np.matmul(L, epsilon)\n",
    "        g       = np.zeros([x_dim, mb_size_])\n",
    "\n",
    "        g[present_idx, :] = g1\n",
    "        g[remove_idx, :]  = g2\n",
    "\n",
    "    m = (1/2 * (1 + scipy.special.erf(g/np.sqrt(2)) ) < pi_).astype(float).T    \n",
    "    return m\n",
    "\n",
    "\n",
    "def copula_generation(mb_size_):\n",
    "    if len(remove_idx) == 0:\n",
    "        epsilon = np.random.normal(loc=0., scale=1., size=[np.shape(L)[0], mb_size_])\n",
    "        g       = np.matmul(L, epsilon)\n",
    "    else:\n",
    "        present_idx = [i for i in range(x_dim) if i not in remove_idx]\n",
    "        epsilon     = np.random.normal(loc=0., scale=1., size=[np.shape(L)[0], mb_size_])\n",
    "        g2      = np.random.normal(loc=0., scale=1., size=[len(remove_idx), mb_size_])\n",
    "        g1      = np.matmul(L, epsilon)\n",
    "        g       = np.zeros([x_dim, mb_size_])\n",
    "\n",
    "        g[present_idx, :] = g1\n",
    "        g[remove_idx, :]  = g2\n",
    "\n",
    "    return g.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed    = 1234 \n",
    "p         = 10\n",
    "sigma_n = 1.0\n",
    "tr_X, tr_Y, tr_Y_onehot = get_noisy_two_moons(n_samples=1000, n_feats=p, noise_twomoon=0.1, noise_nuisance=sigma_n, seed_=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## self-supervised 방식이므로, 라벨된 데이터의 개수를 10개로 지정. \n",
    "max_labeled_samples=10\n",
    "idx1 = random.sample(np.where(tr_Y==1)[0].tolist(), max_labeled_samples)\n",
    "idx0 = random.sample(np.where(tr_Y==0)[0].tolist(), max_labeled_samples)\n",
    "idx=idx1+idx0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_blockcorr(tr_X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SEFS(nn.Module):\n",
    "    def __init__(self, input_dim, z_dim, h_dim, num_layers, dropout):\n",
    "        super(SEFS, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.fc = nn.Linear(input_dim   , h_dim)\n",
    "        ## want to add 3 layers for the fc\n",
    "        ## writh thre fully connected layer at once \n",
    "        ## want activation functiopn between each layer\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, h_dim),\n",
    "            ## activation function\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.xhatdecoder = nn.Sequential(\n",
    "            nn.Linear( z_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, input_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.maskdecoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, input_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        ## fc with outputlayer size \n",
    "        self.fc_out = nn.Linear(h_dim, z_dim)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x=self.encoder(x)\n",
    "        x=self.fc_out(x)\n",
    "        return x\n",
    "    def xhatdecode(self,x):\n",
    "        x=self.xhatdecoder(x)\n",
    "        return x\n",
    "\n",
    "    def maskdecode(self,x):\n",
    "        x=self.maskdecoder(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x): \n",
    "        x=self.encode(x)\n",
    "        xtilde=self.xhatdecode(x)\n",
    "        mask=self.maskdecode(x)\n",
    "        return x,xtilde, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model=SEFS(10000, 10, 100, 3, 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## torchsummary\n",
    " ## deivce selection\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torchsummary import summary\n",
    "summary(model.to(device), (10,10000)) ## input size: sameplesize*feature size\n",
    "\n",
    "## torchsummaryX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy\n",
    "import numpy as np\n",
    "def Gaussian_CDF(x):\n",
    "    return 0.5 * (1. + torch.erf(x / torch.sqrt(2.)))\n",
    "\n",
    "class FCNet(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_layers=1, hidden_features=100,\n",
    "                 activation=nn.ReLU, dropout=0.0):\n",
    "        super(FCNet, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(in_features, hidden_features))\n",
    "            self.layers.append(activation())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_features, out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SEFS_SS_Phase(nn.Module):\n",
    "    def __init__(self, input_dims, network_settings):\n",
    "        super(SEFS_SS_Phase, self).__init__()\n",
    "\n",
    "        self.x_dim = input_dims['x_dim']\n",
    "        self.z_dim = input_dims['z_dim']\n",
    "\n",
    "        self.x_hat=network_settings['x_hat'] ##computed beforhand\n",
    "        self.pi_ = network_settings['pi_'] ## selected beforhand\n",
    "        self.LT = network_settings['LT']     ##  computed beforehand\n",
    "        self.batch_size = network_settings['batch_size']  ## selected beforhand\n",
    "        self.reg_scale = network_settings['reg_scale']\n",
    "        self.h_dim_e = network_settings['h_dim_e']\n",
    "        self.num_layers_e = network_settings['num_layers_e']\n",
    "        self.h_dim_d = network_settings['h_dim_d']\n",
    "        self.num_layers_d = network_settings['num_layers_d']\n",
    "        self.fc_activate_fn = network_settings['fc_activate_fn']\n",
    "\n",
    "        self.encoder = FCNet(self.x_dim, self.z_dim, self.num_layers_e, self.h_dim_e,\n",
    "                             self.fc_activate_fn)\n",
    "        self.decoder_x = FCNet(self.z_dim, self.x_dim, self.num_layers_d, self.h_dim_d,\n",
    "                                self.fc_activate_fn)\n",
    "        self.decoder_m = FCNet(self.z_dim, self.x_dim, self.num_layers_d, self.h_dim_d,\n",
    "                                self.fc_activate_fn)\n",
    "        \n",
    "    def sample_gate_vector(self,x):\n",
    "        # x: (batch_size, x_dim)\n",
    "        # LT_matrix: (x_dim, x_dim) , Lower triangel of correlation matrix should be computed before the traing phase.(via choleskly decompostion) \n",
    "        # batch_size: batch_size\n",
    "        # pi_: (x_dim, 1) , pi_ is a hyper parameter that controls the probability,\n",
    "        ## given correlateion matrix, sample a binary vector from a multivariate Bernoulli distribution\n",
    "        \n",
    "        mask=self.mask_generation(self.pi_, self.LT, self.batch_size)\n",
    "        mask=torch.from_numpy(mask).to(device).float()\n",
    "        x_tilde=mask*x+ (1-mask)*self.x_hat\n",
    "\n",
    "        ## 애매한건 다 네트워크 입력값에 넣는걸로 해놨음. ex batchsize, x_hat(평균값), pi_ 등등\n",
    "\n",
    "\n",
    "        return x_tilde,mask\n",
    "\n",
    "\n",
    "    \n",
    "    def mask_generation(self, pi_, L, batch_size):\n",
    "        ## mb_size is a size of minibatch, pi_ as a hyper parameter that controls the probability, \n",
    "        epsilon = np.random.normal(loc=0., scale=1., size=[np.shape(L)[0], batch_size])\n",
    "        g=np.matmul(L, epsilon)\n",
    "        m = (1/2 * (1 + scipy.special.erf(g/np.sqrt(2)) ) < pi_).astype(float).T\n",
    "        return m\n",
    "        # generate a mask matrix\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # sample a binary vector from \n",
    "        x_tilde,mask=self.sample_gate_vector(x) ## xtilde and 원본 마스크에 대한 정보 저장\n",
    "        z = self.encoder(x_tilde)\n",
    "        x_hat = self.decoder_x(z) ## xtilde로 부터 복원된 x_hat\n",
    "        m_hat = self.decoder_m(z) ## mask로부터 복원된 m_hat\n",
    "\n",
    "        loss_recon = F.mse_loss(x_hat, x, reduction='none')\n",
    "        loss_cross_entropy=F.binary_cross_entropy(m_hat, mask, reduction='none')\n",
    "\n",
    "        return loss_recon,loss_cross_entropy, x_hat, m_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (100) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchsummary\u001b[39;00m \u001b[39mimport\u001b[39;00m summary\n\u001b[0;32m     17\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m summary(model, (\u001b[39m100\u001b[39;49m,\u001b[39m100\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Junho\\envs\\sudal\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[39m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[39m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m model(\u001b[39m*\u001b[39;49mx)\n\u001b[0;32m     74\u001b[0m \u001b[39m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[1;32mc:\\Junho\\envs\\sudal\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[13], line 83\u001b[0m, in \u001b[0;36mSEFS_SS_Phase.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     82\u001b[0m     \u001b[39m# sample a binary vector from \u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     x_tilde,mask\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample_gate_vector(x) \u001b[39m## xtilde and 원본 마스크에 대한 정보 저장\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x_tilde)\n\u001b[0;32m     85\u001b[0m     x_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder_x(z) \u001b[39m## xtilde로 부터 복원된 x_hat\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 63\u001b[0m, in \u001b[0;36mSEFS_SS_Phase.sample_gate_vector\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     61\u001b[0m mask\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_generation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpi_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLT, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m     62\u001b[0m mask\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfrom_numpy(mask)\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m---> 63\u001b[0m x_tilde\u001b[39m=\u001b[39mmask\u001b[39m*\u001b[39;49mx\u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mmask)\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_hat\n\u001b[0;32m     65\u001b[0m \u001b[39m## 애매한건 다 네트워크 입력값에 넣는걸로 해놨음. ex batchsize, x_hat(평균값), pi_ 등등\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m x_tilde,mask\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (100) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "## create dictionary named network_settings\n",
    "## device\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_dims={'x_dim':100, 'z_dim':100}\n",
    "network_settings={'batch_size':100, 'reg_scale':0.5, 'h_dim_e':100, 'num_layers_e':3, 'h_dim_d':100, 'num_layers_d':3, 'fc_activate_fn':nn.ReLU, 'x_hat':torch.randn(100,1), 'pi_':np.random.rand(100,1), 'LT':np.random.rand(100,100)}\n",
    "model=SEFS_SS_Phase(input_dims, network_settings).to(device)\n",
    "\n",
    "## torchsummary\n",
    "    ## deivce selection\n",
    "## want to import torchsummary\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "summary(model, (100,100)) ## input size: sameplesize*feature size\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import special"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sudal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
