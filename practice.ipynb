{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100개의 feature을 통해서 label y를 예측하기 위한 프로세스 마련. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.datasets import make_moons\n",
    "def get_noisy_two_moons(n_samples=1000, n_feats=100, noise_twomoon=0.1, noise_nuisance=1.0, seed_=1234):\n",
    "    X, Y = make_moons(n_samples=n_samples, noise=noise_twomoon, random_state=seed_)\n",
    "    np.random.seed(seed_)\n",
    "    N = np.random.normal(loc=0., scale=noise_nuisance, size=[n_samples, n_feats-2])\n",
    "    X = np.concatenate([X, N], axis=1)\n",
    "\n",
    "    Y_onehot = np.zeros([n_samples, 2])\n",
    "    Y_onehot[Y == 0, 0] = 1\n",
    "    Y_onehot[Y == 1, 1] = 1\n",
    "\n",
    "    return X, Y, Y_onehot\n",
    "\n",
    "##function that arbritrally adds up the feature (there are only two important features but, in order to check for the robustness of the method,  add up all the features)\n",
    "def get_blockcorr(X, block_size=10, noise_=0.5, seed_=1234): \n",
    "    '''\n",
    "        noise 0.5 ~ 0.85 correlation\n",
    "        noise 1.0 ~ 0.66 correlation\n",
    "    '''\n",
    "    for p in range(X.shape[1]):\n",
    "        np.random.seed(seed_ + p)\n",
    "        tmp   = X[:, [p]] + np.random.normal(loc=0., scale=noise_, size=[X.shape[0], block_size-1])\n",
    "\n",
    "        if p == 0:\n",
    "            X_new = np.concatenate([X[:, [p]], tmp], axis=1)\n",
    "        else:\n",
    "            X_new = np.concatenate([X_new, X[:, [p]], tmp], axis=1)    \n",
    "    return X_new   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed    = 1234 \n",
    "p         = 10\n",
    "sigma_n = 1.0\n",
    "tr_X, tr_Y, tr_Y_onehot = get_noisy_two_moons(n_samples=1000, n_feats=p, noise_twomoon=0.1, noise_nuisance=sigma_n, seed_=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## self-supervised 방식이므로, 라벨된 데이터의 개수를 10개로 지정. \n",
    "max_labeled_samples=10\n",
    "idx1 = random.sample(np.where(tr_Y==1)[0].tolist(), max_labeled_samples)\n",
    "idx0 = random.sample(np.where(tr_Y==0)[0].tolist(), max_labeled_samples)\n",
    "idx=idx1+idx0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_blockcorr(tr_X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SEFS(nn.Module):\n",
    "    def __init__(self, input_dim, z_dim, h_dim, num_layers, dropout):\n",
    "        super(SEFS, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.fc = nn.Linear(input_dim   , h_dim)\n",
    "        ## want to add 3 layers for the fc\n",
    "        ## writh thre fully connected layer at once \n",
    "        ## want activation functiopn between each layer\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, h_dim),\n",
    "            ## activation function\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.xhatdecoder = nn.Sequential(\n",
    "            nn.Linear( z_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, input_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.maskdecoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, input_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        ## fc with outputlayer size \n",
    "        self.fc_out = nn.Linear(h_dim, z_dim)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x=self.encoder(x)\n",
    "        x=self.fc_out(x)\n",
    "        return x\n",
    "    def xhatdecode(self,x):\n",
    "        x=self.xhatdecoder(x)\n",
    "        return x\n",
    "\n",
    "    def maskdecode(self,x):\n",
    "        x=self.maskdecoder(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x): \n",
    "        x=self.encode(x)\n",
    "        xtilde=self.xhatdecode(x)\n",
    "        mask=self.maskdecode(x)\n",
    "        return x,xtilde, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model=SEFS(10000, 10, 100, 3, 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (20x100 and 10x10000)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchsummary\u001b[39;00m \u001b[39mimport\u001b[39;00m summary\n\u001b[1;32m----> 6\u001b[0m summary(model\u001b[39m.\u001b[39;49mto(device), (\u001b[39m10\u001b[39;49m,\u001b[39m10000\u001b[39;49m)) \u001b[39m## input size: sameplesize*feature size\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m## torchsummaryX\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\sudal\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[39m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[39m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m model(\u001b[39m*\u001b[39;49mx)\n\u001b[0;32m     74\u001b[0m \u001b[39m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\sudal\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[37], line 54\u001b[0m, in \u001b[0;36mSEFS.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x): \n\u001b[0;32m     53\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode(x)\n\u001b[1;32m---> 54\u001b[0m     xtilde\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxhatdecode(x)\n\u001b[0;32m     55\u001b[0m     mask\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaskdecode(x)\n\u001b[0;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m x,xtilde, mask\n",
      "Cell \u001b[1;32mIn[37], line 45\u001b[0m, in \u001b[0;36mSEFS.xhatdecode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mxhatdecode\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m---> 45\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxhatdecoder(x)\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\sudal\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\sudal\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\sudal\\lib\\site-packages\\torch\\nn\\modules\\module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1209\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1210\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[1;32m-> 1212\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\sudal\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (20x100 and 10x10000)"
     ]
    }
   ],
   "source": [
    "## torchsummary\n",
    " ## deivce selection\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torchsummary import summary\n",
    "summary(model.to(device), (10,10000)) ## input size: sameplesize*feature size\n",
    "\n",
    "## torchsummaryX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sudal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
